{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lab2: Spark SQL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/home/zaranik/.sdkman/candidates/spark/current\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ініціалізація SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FrogDataAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завантаження схеми даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "\n",
    "frog_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"pres.abs\", IntegerType(), False),\n",
    "    StructField(\"northing\", IntegerType(), False),\n",
    "    StructField(\"easting\", IntegerType(), False),\n",
    "    StructField(\"altitude\", IntegerType(), False),\n",
    "    StructField(\"distance\", IntegerType(), False),\n",
    "    StructField(\"NoOfPools\", IntegerType(), False),\n",
    "    StructField(\"NoOfSites\", IntegerType(), False),\n",
    "    StructField(\"avrain\", DoubleType(), False),\n",
    "    StructField(\"meanmin\", DoubleType(), False),\n",
    "    StructField(\"meanmax\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"./frogs.csv\", header=True, schema=frog_schema)\n",
    "df = df.withColumnRenamed(\"pres.abs\", \"presence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створення таблиці та вивід перших 5 рядків на екран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"Frogs\")\n",
    "spark.sql(\"SELECT * from Frogs\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання 1: Створення нової таблиці з двома стовпчиками: distance та массив NoOfPools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синтаксис Spark SQL дещо відрізняється від стандартного SQl, оскільки має набір функій для роботи з массивами: collect_list, array_distinct, array_max та інші."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task1 = spark.sql(\"\"\"\n",
    "    SELECT distance, collect_list(NoOfPools) AS NoOfPools\n",
    "    FROM Frogs\n",
    "    GROUP BY distance\n",
    "    ORDER BY distance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створення таблиці та виведення її"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task1.createOrReplaceTempView(\"Task1\")\n",
    "spark.sql(\"SELECT * from Task1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання 2: Взяти таблицю з завдання 1 і перетворити другий стовпець на кілька\n",
    "стовпців."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знаходження максимальної довжини масиву, вона потрібна підрахунку кількості колонок у новй тимчасовій таблиці"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = df_task1.select(F.size(\"NoOfPools\")).agg(F.max(\"size(NoOfPools)\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створюємо тимчасову таблицю, розвертаючи массив на кілька колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = \", \".join([f\"NoOfPools[{i}] AS Pool_{i}\" for i in range(max_length)])\n",
    "df_task2 = spark.sql(f\"\"\"\n",
    "    SELECT distance, {columns}\n",
    "    FROM Task1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створення таблиці та виведення її"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task2.createOrReplaceTempView(\"Task2\")\n",
    "spark.sql(\"SELECT * from Task2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання 3: Знайти мінімальну висоту для усіх можливих поєднань параметрів\n",
    "pres.abs та NoOfSites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достатньо простих запит на group by, що викоритсовує функцію MIN по полю altitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task3 = spark.sql(\"\"\"\n",
    "    SELECT presence, NoOfSites, MIN(altitude) AS min_altitude\n",
    "    FROM Frogs\n",
    "    GROUP BY presence, NoOfSites\n",
    "\"\"\")\n",
    "df_task3.createOrReplaceTempView(\"Task3\")\n",
    "spark.sql(\"SELECT * from Task3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання 4: Додати в таблицю frogs новий стовпець, що містить різницю між\n",
    "максимальною та поточною відстанню до поселення в залежності\n",
    "від того чи спостерігались жаби. Наприклад, якщо максимальна до\n",
    "поселення від місця, де спостерігались жаби 1000, а в першому\n",
    "рядку відстань 800 і жаби спостерігались, то значення в\n",
    "додатковому стовпчику = 1000-800=200. Зберегти новий файл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визначаємо максимальне значення distance для жаб "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distance = df.where(\"presence = 1\").agg(F.max(\"distance\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Додаємо новий стовпчик з різницею між\n",
    "максимальною та поточною відстанню до поселення в залежності\n",
    "від того чи спостерігались жаби. За умовой задачі якщо presence == 0, то жаби спостерігаються, а якщо 1 - то ні"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task4 = spark.sql(f\"\"\"\n",
    "    SELECT *, \n",
    "           CASE WHEN presence = 0 THEN {max_distance} - distance ELSE NULL END AS distance_diff\n",
    "    FROM Frogs\n",
    "\"\"\")\n",
    "df_task4.createOrReplaceTempView(\"Task4\")\n",
    "spark.sql(\"SELECT * from Task4\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Збереження кінцевої таблиці до файлу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task4.write.csv(\"./modified_frogs.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
