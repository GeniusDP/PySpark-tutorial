{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lab4: Об’єднання потоків в Spark Streaming</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col, desc\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, DoubleType    \n",
    "import os\n",
    "\n",
    "os.environ['SPARK_HOME'] = \"/home/zaranik/.sdkman/candidates/spark/current\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створення Spark сессії"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"RunningCountExample\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опис схеми даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"entity\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"human_food\", DoubleType(), True),\n",
    "    StructField(\"animal_feed\", DoubleType(), True),\n",
    "    StructField(\"processed\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створення статичного датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = [\n",
    "  (1, \"Americas\",\"AMCS\", 1961, 7000, 96000, 11656000), \n",
    "  (2, \"Argentina\", \"ARG\", 2004, 0, 0, 24229000), \n",
    "  (3, \"Asia\", \"ASIA\", 1984, 5954000, 520000, 11831000)\n",
    "]\n",
    "columns = [\"id\", \"entity\", \"code\", \"year\", \"human_food\", \"animal_feed\", \"processed\"]\n",
    "static_df = spark.createDataFrame(static_data, columns)\n",
    "static_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створення спарк стріма, який читає усі .json файли вказаної директорії"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = spark.readStream.schema(schema).json(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Джоін статичного датафрейму та датафрейму стріма, що читає з файлу.\n",
    "Джоін відбувається за полем id. Тобто, у результуючий датафрейм попадуть лише ті записи stream_df, id яких є у static_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = stream_df.join(static_df, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обираємо поля, які хочемо бачити у фінальному стрімі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for the final result\n",
    "result = merged_df.select(\n",
    "    stream_df['id'],\n",
    "    stream_df['entity'],\n",
    "    stream_df['code'],\n",
    "    stream_df['year'],\n",
    "    stream_df['human_food'],\n",
    "    static_df['animal_feed'],\n",
    "    static_df['processed']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записуємо дані стріма у папку output_task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to JSON\n",
    "query = result.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"./output_task1/\") \\\n",
    "    .option(\"checkpointLocation\", \"./output_task1/checkpoints/\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
