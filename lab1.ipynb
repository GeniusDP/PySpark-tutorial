{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lab1: Базове знайомство з Apache Spark</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/home/zaranik/.sdkman/candidates/spark/current\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Завдання 1: Створити кілька своїх DataFrames. Виконати над ними операції\n",
    "об’єднання, додавання та видалення стовпців і рядків </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"lab1\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Створення DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame([('Alice', 1), ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [{'name': 'Alice', 'age': 1}, {'name': 'Bogdan', 'age': 22}]\n",
    "spark.createDataFrame(d).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [\n",
    "  (1, 'Alice', 25),\n",
    "  (2, 'Bob', 30),\n",
    "  (3, 'Charlie', 35)\n",
    "]\n",
    "df1 = spark.createDataFrame(data1, [\"id\", \"name\", \"age\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [\n",
    "  (4, 'David', 40),\n",
    "  (5, 'Eva', 45),\n",
    "  (3, 'Charlie', 35)\n",
    "]\n",
    "df2 = spark.createDataFrame(data2, [\"id\", \"name\", \"age\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Операція об'єднання DataFrames (Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df = df1.union(df2).distinct()\n",
    "union_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Додавання стовпця"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_with_new_column = union_df.withColumn(\"country\", lit(\"Unknown\"))\n",
    "df_with_new_column.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Видалення стовпця"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видаляємо стовпець \"Age\"\n",
    "df_without_column = df_with_new_column.drop(\"age\")\n",
    "df_without_column.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Видалення рядка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видаляємо рядок з ID = 2\n",
    "df_without_row = df_with_new_column.filter(df_with_new_column[\"id\"] != 2)\n",
    "df_without_row.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Додавання рядка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = spark.createDataFrame([(6, \"Bogdan\", 22, \"Ukraine\")], [\"id\", \"name\", \"age\", \"country\"])\n",
    "df_with_new_row = df_with_new_column.union(new_row)\n",
    "df_with_new_row.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Завдання 2: Створити свою схему та завантажити дані з файлу, виконати дії\n",
    "відповідно до варіанту </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "spark = SparkSession.builder.appName(\"lab1-frogs\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the data\n",
    "frog_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"pres.abs\", IntegerType(), False),\n",
    "    StructField(\"northing\", IntegerType(), False),\n",
    "    StructField(\"easting\", IntegerType(), False),\n",
    "    StructField(\"altitude\", IntegerType(), False),\n",
    "    StructField(\"distance\", IntegerType(), False),\n",
    "    StructField(\"NoOfPools\", IntegerType(), False),\n",
    "    StructField(\"NoOfSites\", IntegerType(), False),\n",
    "    StructField(\"avrain\", DoubleType(), False),\n",
    "    StructField(\"meanmin\", DoubleType(), False),\n",
    "    StructField(\"meanmax\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frogs_df = spark.read.csv(\"./frogs.csv\", header=True, schema=frog_schema)\n",
    "frogs_df = frogs_df.withColumnRenamed(\"pres.abs\", \"presence\")\n",
    "frogs_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Знайти максимальну висоту, на якій спостерігаються жаби."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_altitude = frogs_df.filter(frogs_df[\"presence\"] == 1).agg({\"altitude\": \"max\"})\n",
    "max_altitude.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вивести всі висоти, де середні максимальні температури не\n",
    "перевищують 14 градусів і на яких спостерігались жаби."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = frogs_df.filter((frogs_df[\"presence\"] == 1) & (frogs_df[\"meanmax\"] <= 14))\n",
    "df_filtered.select(\"altitude\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Відсортувати місця, де спостерігались жаби, за відстанню до\n",
    "найближчого поселення (спадаючий порядок)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frogs_df.filter(frogs_df[\"presence\"] == 1)\n",
    "df_sorted = df_filtered.orderBy(frogs_df[\"distance\"].desc())\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Порахувати загальну кількість груп для розмноження."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "total_pools = frogs_df.agg(sum(\"NoOfPools\"))\n",
    "print(\"Загальна кількість груп для розмноження:\\n\")\n",
    "total_pools.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
