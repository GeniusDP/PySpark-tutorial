{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lab7: Задачі кластеризації в Spark MLlib</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ['SPARK_HOME'] = \"/home/zaranik/.sdkman/candidates/spark/current\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створення Spark-сессії"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLLib\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задання схеми даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"week_ending\", StringType(), True),\n",
    "    StructField(\"week_number\", IntegerType(), True),\n",
    "    StructField(\"weekly_gross_overall\", IntegerType(), True),\n",
    "    StructField(\"show\", StringType(), True),\n",
    "    StructField(\"theatre\", StringType(), True),\n",
    "    StructField(\"weekly_gross\", IntegerType(), True),\n",
    "    StructField(\"potential_gross\", StringType(), True),  # NA is treated as StringType\n",
    "    StructField(\"avg_ticket_price\", DoubleType(), True),\n",
    "    StructField(\"top_ticket_price\", StringType(), True),  # NA is treated as StringType\n",
    "    StructField(\"seats_sold\", IntegerType(), True),\n",
    "    StructField(\"seats_in_theatre\", IntegerType(), True),\n",
    "    StructField(\"pct_capacity\", DoubleType(), True),\n",
    "    StructField(\"performances\", IntegerType(), True),\n",
    "    StructField(\"previews\", IntegerType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зчитування даних з файлу csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./data/grosses.csv\", header=True, schema=schema)\n",
    "df = df.na.fill({\n",
    "    \"weekly_gross\": 0.0,\n",
    "    \"weekly_gross_overall\": 0.0,\n",
    "    \"pct_capacity\": 0.0,\n",
    "    \"performances\": 0.0,\n",
    "    \"seats_sold\": 0.0\n",
    "})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Step 1: Define the input columns for clustering\n",
    "input_columns = [\"weekly_gross\", \"weekly_gross_overall\", \"pct_capacity\", \"performances\", \"seats_sold\"]\n",
    "\n",
    "# Step 2: Create a vector column 'features'\n",
    "vector_assembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
    "data = vector_assembler.transform(df).select(\"features\", \"show\")  # Replace 'show' with the appropriate column\n",
    "\n",
    "# Step 3: Show the transformed data\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Step 1: Configure K-Means clustering\n",
    "kmeans = KMeans(featuresCol=\"features\", k=3, seed=42)\n",
    "\n",
    "# Step 2: Train the K-Means model\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "# Step 3: Predict clusters for the data\n",
    "predictions = model.transform(data)\n",
    "\n",
    "# Step 4: Evaluate the clustering model\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "\n",
    "# Step 5: Show the predictions\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Score використовується для оцінки якості кластеризації (чим ближче до 1, тим кращий результат). Оскільки початковий результат досить далекий від 1, спробуємо покращити його, збільшивши кількість кластерів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Step 1: Configure K-Means clustering with more clusters\n",
    "kmeans = KMeans(featuresCol=\"features\", k=9, seed=42)  # Increased k to 9\n",
    "\n",
    "# Step 2: Train the K-Means model\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "# Step 3: Predict clusters for the data\n",
    "predictions = model.transform(data)\n",
    "\n",
    "# Step 4: Evaluate the clustering model\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "\n",
    "# Step 5: Show the predicted clusters\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормалізуємо дані"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Step 1: Normalize and scale the data\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "scaled_data = scaler.fit(data).transform(data)\n",
    "\n",
    "# Step 2: Apply K-Means clustering with scaled features\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\", k=3, seed=42)\n",
    "model = kmeans.fit(scaled_data)\n",
    "\n",
    "# Step 3: Predict clusters for the data\n",
    "predictions = model.transform(scaled_data)\n",
    "\n",
    "# Step 4: Evaluate the clustering model\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "\n",
    "# Step 5: Show the predictions\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
