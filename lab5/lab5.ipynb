{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lab5: Попередня обробка даних в Spark MLlib</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ['SPARK_HOME'] = \"/home/zaranik/.sdkman/candidates/spark/current\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створення Spark-сессії"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLLib\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задання схеми даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"week_ending\", StringType(), True),\n",
    "    StructField(\"week_number\", IntegerType(), True),\n",
    "    StructField(\"weekly_gross_overall\", IntegerType(), True),\n",
    "    StructField(\"show\", StringType(), True),\n",
    "    StructField(\"theatre\", StringType(), True),\n",
    "    StructField(\"weekly_gross\", IntegerType(), True),\n",
    "    StructField(\"potential_gross\", StringType(), True),  # NA is treated as StringType\n",
    "    StructField(\"avg_ticket_price\", DoubleType(), True),\n",
    "    StructField(\"top_ticket_price\", StringType(), True),  # NA is treated as StringType\n",
    "    StructField(\"seats_sold\", IntegerType(), True),\n",
    "    StructField(\"seats_in_theatre\", IntegerType(), True),\n",
    "    StructField(\"pct_capacity\", DoubleType(), True),\n",
    "    StructField(\"performances\", IntegerType(), True),\n",
    "    StructField(\"previews\", IntegerType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зчитування даних з файлу csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./data/grosses.csv\", header=True, schema=schema)\n",
    "df = df.na.fill({\"weekly_gross\": 0.0})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Find the maximum and minimum values for 'weekly_gross'\n",
    "max_d = df.select(SF.max(\"weekly_gross\")).collect()[0][0]\n",
    "min_d = df.select(SF.min(\"weekly_gross\")).collect()[0][0]\n",
    "\n",
    "print(\"min_d = \", min_d)\n",
    "print(\"max_d = \", max_d)\n",
    "\n",
    "# Define the number of intervals (borders)\n",
    "borders_count = 10\n",
    "\n",
    "# Calculate the border values for equal intervals\n",
    "borders = [min_d + (max_d - min_d) / (borders_count - 1) * i for i in range(borders_count)]\n",
    "print(borders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Bucketizer, QuantileDiscretizer\n",
    "\n",
    "# Apply Bucketizer to group data into specified intervals\n",
    "bucketer = Bucketizer(splits=borders, inputCol=\"weekly_gross\", outputCol=\"weekly_gross_bucket\")\n",
    "bucketed_data = bucketer.transform(df)\n",
    "\n",
    "# Apply QuantileDiscretizer for automatic quantile-based grouping\n",
    "quantizer = QuantileDiscretizer(numBuckets=borders_count, inputCol=\"weekly_gross\", outputCol=\"quantized_weekly_gross\")\n",
    "quantized_data = quantizer.fit(df).transform(bucketed_data)\n",
    "\n",
    "# Show the result\n",
    "quantized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "\n",
    "# Step 1: Assemble 'weekly_gross' into a vector column\n",
    "assembler = VectorAssembler(inputCols=[\"weekly_gross\"], outputCol=\"vector_data\")\n",
    "vector_data = assembler.transform(df)\n",
    "\n",
    "# Step 2: Scale the vector column using MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"vector_data\", outputCol=\"scaled_features\")\n",
    "scaled_data = scaler.fit(vector_data).transform(vector_data)\n",
    "\n",
    "# Show the scaled data\n",
    "scaled_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Convert the 'weekly_gross' column into a numerical index\n",
    "indexer = StringIndexer(inputCol=\"weekly_gross\", outputCol=\"weekly_gross_index\")\n",
    "indexed_data = indexer.fit(df).transform(df)\n",
    "\n",
    "# Show the transformed data\n",
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завдання 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "# Step 1: Convert 'weekly_gross' to StringType\n",
    "df = df.withColumn(\"weekly_gross_str\", col(\"weekly_gross\").cast(\"string\"))\n",
    "\n",
    "# Step 2: Tokenize the 'weekly_gross_str' column\n",
    "tokens = Tokenizer().setInputCol(\"weekly_gross_str\").setOutputCol(\"weekly_gross_tokens\")\n",
    "tokenized_data = tokens.transform(df)\n",
    "\n",
    "# Step 3: Remove stop-words\n",
    "english_stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover().setStopWords(english_stop_words) \\\n",
    "                          .setInputCol(\"weekly_gross_tokens\") \\\n",
    "                          .setOutputCol(\"filtered_weekly_gross_tokens\")\n",
    "filtered_data = stops.transform(tokenized_data)\n",
    "\n",
    "# Step 4: Apply CountVectorizer\n",
    "cv = CountVectorizer().setInputCol(\"filtered_weekly_gross_tokens\") \\\n",
    "                      .setOutputCol(\"count_vector\").setVocabSize(500)\n",
    "fitted_cv = cv.fit(filtered_data)\n",
    "count_vectorized_data = fitted_cv.transform(filtered_data)\n",
    "\n",
    "# Step 5: Display the result\n",
    "count_vectorized_data.show(10000, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
